Artificial Intelligence:  Natural Language Processing
1
Artificial Intelligence:
Introduction to
Natural Language Processing
Menu
1. Introduction
2. Bag of word model
3. n-gram models
4. Linguistic features for NLP
ï® 2
Languages
ï® Artificial
ï± Smaller vocabulary
ï± Simple syntactic structures
ï± Non-ambiguous
ï± Not tolerant to errors (ex. Syntax error)
ï® Natural
ï± Large and open vocabulary (new words everyday)
ï± Complex syntactic structures
ï± Very ambiguous
ï± Robust (ex. forgot a comma, a wordâ€¦ still OK)
ï® 3
Question Answering: IBMâ€™s Watson
ï® Won Jeopardy on February 16, 2011!
4
WILLIAM WILKINSONâ€™S
â€œAN ACCOUNT OF THE PRINCIPALITIES OF
WALLACHIA AND MOLDOVIAâ€
INSPIRED THIS AUTHORâ€™S
MOST FAMOUS NOVEL
Who is Bram
Stoker?
(Dracula)
Information Extraction
Subject: curriculum meeting
Date: January 15, 2012
To: Dan Jurafsky
Hi Dan, weâ€™ve now scheduled the curriculum meeting.
It will be in Gates 159 tomorrow from 10:00-11:30.
-Chris
Create new Calendar entry
Event:  Curriculum mtg
Date:   Jan-16-2012
Start:   10:00am
End:    11:30am
Where: Gates 159
Information Extraction & Sentiment Analysis
nice and compact to carry!
since the camera is small and light, I won't need to carry around
those heavy, bulky professional cameras either!
the camera feels flimsy, is plastic and very light in weight you have
to be very delicate in the handling of this camera
6
Size and weight
Attributes:
zoom
affordability
size and weight
flash
ease of use
âœ“
âœ—
slide from Olga Veksler (U. Western Ontario)
Machine Translation
Fully automatic
7
Helping human translators
Enter Source Text:
Translation from Stanfordâ€™s Phrasal:
è¿™ ä¸è¿‡ æ˜¯ ä¸€ ä¸ª æ—¶é—´ çš„ é—®é¢˜ .
This is only a matter of time.
Where we are today
Part-of-speech (POS) tagging
Named entity recognition (NER)
Sentiment analysis
mostly solved making good progress Good progress by
Deep Learning
Spam detection
Letâ€™s go to Agra!Letâ€™s go to Agra!
Buy V1AGRA â€¦Buy V1AGRA â€¦
Colorless   green   ideas   sleep   furiously.Colorless   green   ideas   sleep   furiously.
ADJ         ADJ    NOUN  VERB      ADV     ADJ         ADJ    NOUN  VERB      ADV
Einstein met with UN officials in PrincetonEinstein met with UN officials in Princeton
PERSON              ORG                      LOCPERSON              ORG                      LOC
Information extraction (IE)
Youâ€™re invited to our
dinner party, Friday May
27 at 8:30
Party
May
27
add
Best roast chicken in San Francisco!Best roast chicken in San Francisco!
The waiter ignored us for 20 minutes.The waiter ignored us for 20 minutes.
Machine translation (MT)
The 13th Shanghai International Film Festivalâ€¦The 13th Shanghai International Film Festivalâ€¦
ç¬¬ 13 å±Šä¸Šæµ·å›½é™…ç”µå½±èŠ‚å¼€å¹•â€¦ç¬¬ 13 å±Šä¸Šæµ·å›½é™…ç”µå½±èŠ‚å¼€å¹•â€¦
Question answering (QA)
Q. How effective is ibuprofen in
reducing fever in patients with acute
febrile illness?
Parsing
I can see Alcatraz from the window!I can see Alcatraz from the window!
Paraphrase
XYZ acquired ABC yesterdayXYZ acquired ABC yesterday
ABC has been taken over by XYZABC has been taken over by XYZ
Summarization
The Dow Jones is upThe Dow Jones is up
Housing prices roseHousing prices rose
Economy
is good
is goodThe S&P500 jumpedThe S&P500 jumped
Coreference resolution
Carter told Mubarak he shouldnâ€™t run again.Carter told Mubarak he shouldnâ€™t run again.
Word sense disambiguation (WSD)
I need new batteries for my mouse.I need new batteries for my mouse.
Dialog  Where is Citizen Kane playing in
SF?
Where is Citizen Kane playing in
Castro Theatre at 7:30.
Do you want a ticket?
ï® Because it is ambiguous:
1. The computer understands you
as well as your mother
understands you.
2. The computer understands that
you like (love) your mother.
3. The computer understands you
as well as it understands your
mother.
Why is NLP hard?
â€œAt last, a computer that understands you like your motherâ€
10
Another Example of Ambiguity
ï± Even simple sentences are highly ambiguous
ï± â€œGet the cat with the glovesâ€
11
And Even More Examples of Ambiguity
ï® Iraqi Head Seeks Arms
ï® Ban on Nude Dancing on Governorâ€™s Desk
ï® Juvenile Court to Try Shooting Defendant
ï® Teacher Strikes Idle Kids
ï® Kids Make Nutritious Snacks
ï® British Left Waffles on Falkland Islands
ï® Red Tape Holds Up New Bridges
ï® Bush Wins on Budget, but More Lies Ahead
ï® Hospitals are Sued by 7 Foot Doctors
ï® Stolen Painting Found by Tree
ï® Local HS Dropouts Cut in Half
ï® Natural Language Processing
= automatic processing of written texts
1. Natural Language Understanding
ï± Input = text
2. Natural Language Generation
ï± Output = text
ï® Speech Processing
= automatic processing of speech
1. Speech Recognition
ï± Input = acoustic signal
2. Speech Synthesis
ï± Output = acoustic signal
NLP vs Speech Processing
ï® 12
Remember these slides?
13
The Ancient Land of NLP (aka GOFAI)
(circa A.D. 1950...mid 1980)
14
The Ancient Land of NLP
Speech
Kingdom
Village of
CS &
Linguists
Information
Retrieval
Forest
Machine
Learning
Island
Rule-based NLP
15
https://image.slidesharecdn.com/maryamsaihbani-161004094341/95/lefttoright-hierarchical-phrasebased-
translation-and-its-application-in-simultaneous-speech-translation-maryam-siahbani-4-638.jpg?cb=1477004310
Symbolic methods / Linguistic approach / Knowledge-rich approach
â€¢ Cognitive approach
â€¢ Rules are developed by hand in collaboration with linguists
1st Invasion of NLP, from ML
(mid 1980 â€“ circa 2010)
16
The Land of Statistical NLP
City of
Statistical NLP
17
Syntactic
parsing
Part-of-
speech
tagging
stemming
tokenisation Decision trees
Statistical methods / Machine Learning / Knowledge-poor method
â€¢ Engineering Approach
â€¢ Rules are developed automatically (using machine learning)
â€¢ But the linguistic features are hand-engineered and fed to the ML model
â€¢ Applications: Information Retrieval, Predictive Text / Word Completion,
Language Identification, Text Classification, Authorship Attribution...
Neural networks
NaÃ¯ve Bayes classifier
K-means clustering
Feature Extraction
(designed by hand)
Machine Learning
Model Applications
18
Applications
linguistic features are hand-engineered and fed to the ML model
2nd Invasion of NLP, by Deep Learning
(circa 2010-today)
19
The Modern Land of
Deep Language Processing
Metropolis
of  Deep
Language
Processing
Deep
20
Deep Neural Networks applied to NLP problems
â€¢ And the linguistic features are found automatically!
ï® 21
22
Bag-of-word Model (BOW)
ï® A simple model where word order is ignored
ï® used in many applications:
ï± NB spam filter seen in class a few weeks ago
ï± Information Retrieval (eg. google search)
ï± ...
ï® But has severe limits to understand meaning of text...
ï® Maybe we should take word order into account...
Word Freq.
Mary 2
apples 1
did 2
eat 1
John 1
kill 1
like 1
not 1
to 1
23
Limits of BOW Model
ï® word order is ignored ==> meaning of text is lost.
ï® n-grams take [a bit of] word order into account
Mary did kill John.
Mary did not like to eat
apples.
John did not kill Mary.
Mary did like to eat apples.
Mary did not like to kill
John.
Mary did eat apples.
...
ï® 24
25
n-gram Model
ï® An n-gram model is a probability distribution over sequences of
events (grams/units/items)
ï® models the order of the events
ï® Used when the past sequence of events is a good indicator of
the next event to occur in the sequence
ï® i.e. To predict the next event in a sequence of event
ï® E.g.:
ï± next move of player based on his/her past moves
ï® left right right up ... up? down? left? right?
ï± next base pair based on past DNA sequence
ï® AGCTTCG ... A? G? C? T?
ï± next word based on past words
ï® Hi dear, how are ... helicopter? laptop? you? magic?
26
Whatâ€™s a Language Model?
ï® A Language model is a n-gram model over word/character
sequences
ï® ie: events = words  or  events = character
ï® P(â€œIâ€™d like a coffee with 2 sugars and milkâ€) â‰ˆ 0.001
ï® P(â€œIâ€™d hike a toffee with 2 sugars and silkâ€) â‰ˆ 0.000000001
Applications of LM
ï® Speech Recognition
ï® Statistical Machine Translation
ï® Language Identification
ï® Spelling correction
ï± He is trying to fine out.
ï± He is trying to find out.
ï® Optical character recognition / Handwriting
recognition
ï® â€¦
In Speech Recognition
ï‚§ Goal: find most likely sentence (S*) given the observed sound (O) â€¦
ï‚§ ie. pick the sentence with the highest probability:
ï® We can use Bayes rule to rewrite this as:
ï® Since denominator is the same for each candidate S, we can ignore it for the
argmax:
Acoustic model --
Probability of the possible
phonemes in the language +
Probability of â‰  pronunciations
Language model -- P(a sentence)
Probability of the candidate
sentence in the language
Given: Observed sound - O
Find: The most likely word/sentence â€“ S*
S1: How to recognize speech.  ?
S2: How to wreck a nice beach. ?
S3: â€¦
O)|P(SargmaxS*
LSïƒ
ï€½
P(S)S)|P(O argmaxS*
LS
ï‚´ï€½
ïƒ
P(O)
S)P(S)|P(OargmaxS*
29
argmax
P(acoustic signal)
P(acoustic signal | word sequence) x P(word sequence)argmax
P(word sequence | acoustic signal)argmax
cewordsequen
ceword sequenc
Acoustic model
Language model
P(acoustic signal | word sequence) x P(word sequence)
In Statistical Machine Translation
ï® Assume we translate from fr[foreign] to English  i.e.: (en|fr)
Given: Foreign sentence - fr
Find: The most likely English
sentence â€“ en*
S1: Translate that!
S2: Translated this!
S3: Eat your soup!
S4â€¦
Translation model
Automatic Language Identificationâ€¦
guess how thatâ€™s done?
P(en) x en)|P(fr argmaxen*
en
31
â€œShannon Gameâ€ (Shannon, 1951)
â€œI am going to make a collect â€¦â€
ï® Predict the next word/character given the n-1 previous
words/characters.
https://en.wikipedia.org/wiki/Claude_Shannon
32
1st approximation
ï® each word has an equal probability to follow any
other
ï± with 100,000 words, the probability of each word at any
given point is .00001
ï® but some words are more frequent then othersâ€¦
ï® â€œtheâ€ appears many more times, than â€œrabbitâ€
33
2nd approximation: unigrams
ï® take into account the frequency of the word in
some training corpus
ï± at any given point, â€œtheâ€  is more probable than â€œrabbitâ€
ï® but does not take word order into account.  This
is the bag of word approach.
ï± â€œJust then, the white â€¦â€
ï® so the probability of a word also depends on the
previous words (the history)
P(wn |w1w2â€¦wn-1)
34
n-grams
ï® â€œthe large green ______ .â€
ï± â€œmountainâ€? â€œtreeâ€?
ï® â€œSue swallowed the large green ______ .â€
ï± â€œpillâ€?  â€œbroccoliâ€?
ï® Knowing that Sue â€œswallowedâ€ helps narrow down
possibilities
ï® i.e., going back 3 words before helps
ï® But, how far back do we look?
35
Bigrams
ï® first-order Markov models
ï® N-by-N matrix of probabilities/frequencies
ï® N = size of the vocabulary we are using
P(wn|wn-1)
1s
t  w
or
d
2nd word
a aardvark aardwolf aback â€¦ zoophyte zucchini
a 0 0 0 0 â€¦ 8 5
aardvark 0 0 0 0 â€¦ 0 0
aardwolf 0 0 0 0 â€¦ 0 0
aback 26 1 6 0 â€¦ 12 2
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
zoophyte 0 0 0 1 â€¦ 0 0
zucchini 0 0 0 3 â€¦ 0 0
36
Trigrams
ï® second-order Markov models
ï® N-by-N-by-N matrix of probabilities/frequencies
P(wn|wn-1wn-2)
3rd word
37
Why use only bi- or tri-grams?
ï® Markov approximation is still costly
with a 20 000 word vocabulary:
ï± bigram needs to store 400 million parameters
ï± trigram needs to store 8 trillion parameters
ï± using a language model > trigram is impractical
38
Building n-gram Models
1. Data preparation:
ï± Decide on training corpus
ï± Clean and tokenize
ï± How do we deal with sentence boundaries?
ï® I eat.  I sleep.
ï± (I eat) (eat I) (I sleep)
ï® <s>I eat <s> I sleep <s>
ï± (<s> I) (I eat) (eat <s>) (<s> I) (I sleep) (sleep <s>)
39
2. Count words and build model
ï± Let C(w1...wn) be the frequency of n-gram w1...wn
3. Smooth your model (see later)
)...wC(w
)...wC(w  )...ww|(wP
1-n1
n1
1-n1n ï€½
40
Example 1:
ï® in a training corpus, we have 10 instances of
â€œcome acrossâ€
ï± 8 times, followed by â€œasâ€
ï± 1 time, followed by â€œmoreâ€
ï± 1 time, followed by â€œaâ€
ï® so we have:
ï±
ï± P(more | come across) = 0.1
ï± P(a | come across) = 0.1
ï± P(X | come across) = 0  where X â‰  â€œasâ€, â€œmoreâ€, â€œaâ€
8
across) C(come
as) across C(come  across) come |P(as ï€½ï€½
â†’ Worksheet #10 (â€œSentence Probabilityâ€)
Remember this slide...
42
43
Some Adjustments
ï® product of probabilitiesâ€¦ numerical underflow
for long sentences
ï® so instead of multiplying the probs, we add the
log of the probs
P(I want to eat British food)
= log(P(I|<s>)) + log(P(want|I)) + log(P(to|want)) + log(P(eat|to)) +
log(P(British|eat)) + log(P(food|British))
= log(.25) + log(.32) + log(.65) + log (.26) + log(.001) + log(.6)
44
Problem: Data Sparseness
ï® What if a sequence never appears in training corpus? P(X)=0
ï± â€œcome across the menâ€ --> prob = 0
ï± â€œcome across some menâ€ --> prob = 0
ï± â€œcome across 3 menâ€ --> prob = 0
ï® The model assigns a probability of zero to unseen events â€¦
ï® probability of an n-gram involving unseen words will be zero!
ï® Solution: smoothing
ï± decrease the probability of previously seen events
ï± so that there is a little bit of probability mass left over for
previously unseen events
Remember this other slide...
45
46
Add-one Smoothing
ï® Pretend we have seen every n-gram at least once
ï® Intuitively:
ï± new_count(n-gram) = old_count(n-gram) + 1
ï® The idea is to give a little bit of the probability
space to unseen events
47
Add-one: Example
unsmoothed bigram counts (frequencies):
ï® Assume a vocabulary of 1616 (different) words
ï± V = {a, aardvark, aardwolf, aback, â€¦ , I, â€¦, want,â€¦ to, â€¦, eat, Chinese, â€¦, food, â€¦, lunch, â€¦,
zoophyte, zucchini}
ï± |V| = 1616 words
ï® And a total of N = 10,000 bigrams (~word instances) in the training corpus
I want to eat Chinese food lunch â€¦ Total
I 8 1087 0 13 0 0 0  C(I)=3437
want 3 0 786 0 6 8 6  C(want)=1215
to 3 0 10 860 3 0 12  C(to)=3256
eat 0 0 2 0 19 2 52  C(eat)=938
Chinese 2 0 0 0 0 120 1  C(Chinese)=213
food 19 0 17 0 0 0 0  C(food)=1506
lunch 4 0 0 0 0 1 0  C(lunch)=459
â€¦         ...
...
N=10,000
48
unsmoothed bigram counts:
unsmoothed bigram conditional probabilities:
â€¦
437 3
8I)|P(I
000 10
8P(II)
:note
49
Add-one: Example (conâ€™t)
add-one smoothed bigram counts:
add-one bigram conditional probabilities:
I 8   9 1087
1088
1 14 1 1 1  3437
C(I) + |V| = 5053
want 3  4 1 787 1 7 9 7  C(want) + |V| = 2831
to 4 1 11 861 4 1 13  C(to) + |V| = 4872
eat 1 1 23 1 20 3 53  C(eat) + |V| = 2554
Chinese 3 1 1 1 1 121 2  C(Chinese) + |V| = 1829
food 20 1 18 1 1 1 1  C(food) + |V| = 3122
lunch 5 1 1 1 1 2 1  C(lunch) + |V| = 2075
â€¦         total = 10,000
N+|V|2 = 10,000 + (1616)2
= 2,621,456
I want to eat Chinese food lunch â€¦
I .0018
(9/5053)
.215 .00019 .0028
.00019 .00019 .00019
want .0014 .00035 .278 .00035 .0025 .0031 .00247
to .00082 .0002 .00226 .1767 .00082 .0002 .00267
eat .00039 .00039 .0009 .00039 .0078 .0012 .0208
â€¦
50
Add-one, more formally
N: size of the corpus
i.e. nb of n-gram tokens in training corpus
B: number of "bins"
i.e. nb of different n-gram types
i.e. nb of cells in the matrix
e.g. for bigrams, it's (size of the vocabulary)2
B  N
1  )w w (w C  )w w (wP n1 2n21Add1
ï€«
ï€«ï‚¼
ï€½ï‚¼
51
Add-delta Smoothing
ï® every previously unseen n-gram is given a low probability
ï® but there are so many of them that too much probability mass is
given to unseen events
ï® instead of adding 1, add some other (smaller) positive value ğ›¿
ï® most widely used value for ğ›¿ = 0.5
ï® better than add-one, but stillâ€¦
B   N
)w w (w C  )w w (wP n1 2n21AddD
ï¤
52
Factors of Training Corpus
ï® Size:
ï± the more, the better
ï± but after a while, not much improvementâ€¦
ï® bigrams (characters) after 100â€™s million words
ï® trigrams (characters) after some billions of words
ï® Genre (adaptation):
ï± training on cooking recipes and testing on aircraft
maintenance manuals
53
Example: Language Identification
ï® hypothesis: texts that resemble
each other (same author, same
language) share similar
character/word sequences
ï± In English character sequence
â€œingâ€  is more probable than in
French
ï® Training phase:
ï± construction of the language
model
ï± with pre-classified documents
(known language/author)
ï® Testing phase:
ï± apply language model to unknown
text
54
ï® bigram of characters
ï± characters = 26 letters (case insensitive)
ï± possible variations: case sensitivity,
punctuation, beginning/end of sentence
marker, â€¦
55
1. Train a character-based language model for Italian:
2. Train a character-based language model for Spanish:
3. Given a unknown sentence â€œche bella cosaâ€  is it in Italian or in Spanish?
P(â€œche bella cosaâ€) with the Italian LM
P(â€œche bella cosaâ€) with the Spanish LM
4. Highest probability  language of sentenceâ†’
A B C D â€¦ Y Z
A 0.0014 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014
B 0.0014 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014
C 0.0014 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014
D 0.0042 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014
E 0.0097 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ 0.0014
Y 0.0014 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014
Z 0.0014 0.0014 0.0014 0.0014 0.0014 0.0014 0.0014
Googleâ€™s Web 1T 5-gram model
ï® 5-grams
ï® generated from 1 trillion words
ï® 24 GB compressed
ï± Number of tokens: 1,024,908,267,229
ï± Number of sentences: 95,119,665,584
ï± Number of unigrams: 13,588,391
ï± Number of bigrams: 314,843,401
ï± Number of trigrams: 977,069,902
ï± Number of fourgrams: 1,313,818,354
ï± Number of fivegrams: 1,176,470,663
ï® See discussion:
http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html
ï® See Google Ngram Viewer: http://en.wikipedia.org/wiki/Google_Ngram_Viewer
56
http://en.wikipedia.org/wiki/Google_Ngram_Viewer
Problem with n-grams
ï® Natural language is not linear ....
ï® there may be long-distance dependencies.
ï± Syntactic dependencies
ï® The man next to the large oak tree near â€¦ is tall.
ï® The men next to the large oak tree near â€¦ are tall.
ï± Semantic dependencies
ï® The bird next to the large oak tree near â€¦ flies rapidly.
ï® The man next to the large oak tree near â€¦ talks rapidly.
ï± World knowledge
ï® Michael Jackson, who was featured in ..., is buried in California.
ï® Michael BublÃ©, who was featured in ..., is living in California.
ï® More complex models of language are needed to handle such
dependencies.
57
ï® 58
Linguistic features used for what?
59
60
Stages of NLU
source: Luger (2005)
61
Parsing (Syntax):
ï® What words are available in a
language?  gfiioudd  / table
ï® How to arrange words
together?
the rose is red /  red the rose is
Semantic interpretation:
ï® Lexical Semantics :
What is the
meaning/semantic
relations between
individual words?
Chair:  person?  Furniture?
ï® Compositional
Semantics: What is the
meaning of phrases and
sentences?
The chairâ€™s leg is broken
62
ï® Discourse Analysis
How to relate the meaning of sentences to
surrounding sentences?
I have to go to the store.  I need butter.
I have to go to the university.  I need butter.
ï® Pragmatics
How people use language in a social
environment?
Do you have a child?
Do you have a quarter?
ï® World Knowledge
How knowledge about the world (history,
facts, â€¦) modifies our understanding of
text?
Bill Gates passed away last night.
63
64
Syntactic Parsing
1. Assign the right part of speech (NOUN, VERB, â€¦) to
individual words in a text
2. Determine how words are put together to form
correct sentences
ï® The/DET rose/NOUN is/VERB red/ADJ.
ï® Is/VERB red/ADJ the/DET rose/NOUN.
65
English Parts-of-Speech
ï® Open (lexical) class words
ï± new words can be added easily
ï± nouns, main verbs, adjectives, adverbs
ï± some languages do not have all these categories
ï® Closed (functional) class words
ï± generally function/grammatical words
ï± aka stop words
ï± ex. the, in, and, over, beyondâ€¦
ï± relatively fixed membership
ï± prepositions, determiners, pronouns, conjunctions, â€¦
Smurf talk on youtube:
https://www.youtube.com/watch?v=7BPx-vl8G00
66
Syntax
ï®  How parts-of-speech are organised into larger
syntactic constituents
ï®  Main Constituents:
ï± S: sentence       The boy is happy.
ï± NP: noun phrase      the little boy from Paris, Sam Smith, I,
ï± VP: verb phrase       eat an apple, sing, leave Paris in the night
ï± PP: prepositional phrase  in the morning, about my ticket
ï± AdjP: adjective phrase    really funny, rather clear
ï± AdvP: adverb phrase slowly, really slowly
67
A Parse Tree
ï® a tree representation of the application
of the grammar to a specific sentence.
68
a CFG consists of
ï® set of non-terminal symbols
ï± constituents & parts-of-speech
ï± S, NP, VP, PP, D, N, V, ...
ï® set of terminal symbols
ï± words & punctuation
ï± cat, mouse, nurses, eat, ...
ï® a non-terminal designated as the starting symbol
ï± sentence S
ï® a set of re-write rules
ï± having a single non-terminal on the LHS and one or more
terminal or non-terminal in the RHS
ï± S --> NP VP
ï± NP --> Pro
ï± NP --> PN
ï± NP --> D N
70
ï® parsing:
ï± goal:
ï® assign syntactic structures to a sentence
ï± result:
ï® (set of) parse trees
ï® we need:
ï± a grammar:
ï® description of the language constructions
ï± a parsing strategy:
ï® how the syntactic analysis are to be computed
â†’ Worksheet #10 (â€œParsingâ€)
71
Parsing Strategies
ï® parsing is seen as a search problem
through the space of all possible parse
trees
ï± bottom-up (data-directed): words --> grammar
ï± top-down (goal-directed): grammar --> words
ï± breadth-first: compute all paths in parallel
ï± depth-first: exhaust 1 path before considering
another
ï± Heuristic search
72
Example: John ate the cat
ï® Bottom-up parsing /
breadth first
1. John ate the cat
2. PN ate the cat
3. PN V the cat
4. PN V ART cat
5. PN V ART N
6. NP V ART N
7. NP V NP
8. NP VP
9. S
ï® Top-down parsing /
depth first
1. S
2. NP VP
3. PN VP
4. John VP
5. John V NP
6. John ate NP
7. John ate ART N
8. John ate the N
9. John ate the cat
73
Depth-first vs Breadth-first
the cat eats the mouse.
ï® depth-first: exhaust 1 path
before considering another
ï® breadth-first:
ï± compute 1 level at a time
ï® Heuristic search:
ï± e.g. preference to shorter rules
Grammar:
(1) S --> NP VP
(2) S --> VP
(3) S --> Aux NP VP
(4) NP --> Det N PP
(5) NP -- > Det N
(6) PP -- > Prep N
â€¦
Lexicon:
(10) Det --> the
(11) N --> cat
(12) VB --> eats
S
NP-VP       VP   Aux-NP-VP
Det-N-PP Det-N â€¦
the  cat Prep-NP
74
Summary of Parsing Strategies
Depth
First
Breath
Heuristic
Search
Top down ïƒ¼ ïƒ¼ ïƒ¼
Bottom up ïƒ¼ ïƒ¼ ïƒ¼
75
Problem: Multiple parses
ï® Many possible parses for a single sentence happens
very oftenâ€¦
ï± Prepositional phrase attachment (PP-attachment)
ï® We painted the wall with cracks.
ï® The man saw the boy with the telescope.
ï® I shot an elephant in my pyjamas.
ï± Conjunctions and appositives
ï® Maddy, my dog, and Samy
-- > (Maddy, my dog), and (Samy)
-- > (Maddy), (my dog), and (Samy)
ï®  These phenomena can quickly increase the number of
possible parse trees!
76
PP attachment:
The man saw the boy with the telescope.
Correct parse 1 Correct parse 2
source: Robert Dale.
77
Probabilistic Parsing
â€œOne morning I shot an elephant in my pyjamas.  How he got
into my pyjamas, I donâ€™t know.â€
G. Marx, Animal Crackers, 1930.
ï® Sentences can be very ambiguousâ€¦
ï± A non-probabilistic parser may find a large set of possible
parses
ï± --> need to pick the most probable parse one from the set
78
Example of a PCFG
ï® Intuitively, P(VP â†’ V NP) is:
ï± the probability of expanding VP by a V NP, as opposed
to any other rules for VP
ï® So for:
ï± VP: âˆ€i âˆ‘i P(VP --> B) = .7 + .3 = 1
ï± NP: âˆ€i âˆ‘i P(NP --> B) = .4 + .1 + .18 + .04 + .18 + .1 = 1
source:  Manning, and SchÃ¼tze, Foundations of Statistical Natural Language Processing, MIT Press (1999)
79
ï® Product of the probabilities of the rules used in subtrees
ï® Ex: â€œAstronomers saw stars with ears.â€
.
P(t1) = 1x.1x.7x1x.4x.18x1x1x.18 P(t2) = 1x.1x.3x.7x1x1x.18x1x.18
= .0009072         = .0006804
Probability of a parse tree
80
81
Semantic Interpretation
ï® Map sentences to some representation of its
meaning
ï± e.g., logics, semantic network, embeddingâ€¦
1. Lexical Semantics
ï® i.e., Meaning of individual words
2. Compositional Semantics
ï® i.e., Meaning of combination of words
82
Lexical Semantics
ï® ie. The meaning of individual words
ï± A word may denote different things (ex. chair)
ï± The meaning/sense of words is not clear-cut
ï± E.g. Overlapping of word senses across languages
leg
patte
Ã©tape
jambe pied
animal
journey
human
chair
83
Word Sense Disambiguation (WSD)
ï® Determining which sense of a word is used
in a specific sentence
ï± I went to the bank of Montreal and deposited 50$.
ï± I went to the bank of the river and dangled my feet.
84
ï® WSD can be viewed as typical classification
problem
ï± use machine learning techniques (ex. NaÃ¯ve Bayes
classifier, decision tree) to train a system
ï± that learns a classifier (a function f) to assign to
unseen examples one of a fixed number of senses
(categories)
ï® Input:
ï± Target word: The word to be disambiguated
ï± Features?
ï® Output:
ï± Most likely sense of the word
WSD as a Classification Problem
85
Features for WSD
ï®  intuition:
ï± sense of a word depends on the sense of surrounding words
ï® ex: bass = fish, musical instrument, ...
ï® So use a window of words around the target word as
features
Surrounding words Most probable sense
â€¦riverâ€¦ fish
â€¦violinâ€¦ instrument
â€¦salmonâ€¦ fish
â€¦playâ€¦ instrument
â€¦playerâ€¦ instrument
â€¦stripedâ€¦ fish
86
ï® Take a window of n words around the target word
ï® Encode information about the words around the target word
ï± An electric guitar and bass player stand off to one side,
not really part of the scene, just as a sort of nod to gringo
expectations perhaps.
87
NaÃ¯ve Bayes WSD
ï® Goal: choose the most probable sense s* for a word given a vector
V of surrounding words
ï® Feature vector V contains:
ï± Features: words [fishing, big, sound, player, fly, rod, â€¦]
ï± Value: frequency of these words in a window before & after the
target word [0, 0, 0, 2, 1, 0, â€¦]
ï® Bayes decision rule:
ï± s* = argmaxsk P(sk|V)
ï± where:
ï® S is the set of possible senses for the target word
ï® sk is a sense in S
ï® V is the feature vector
88
ï® Training a NaÃ¯ve Bayes classifier
= estimating P(vj|sk) and P(sk) from a sense-tagged training
corpus
= finding the most likely sense k
Nb of occurrences of feature j
over the total nb of features
appearing in windows of Sk
Nb of  occurrences of sense k
over nb of all occurrences of
ambiguous word
ïƒ·
ïƒ¸
ïƒ¶
ïƒ§
ïƒ¨
ïƒ¦
ïƒ¥ï€«ï€½
n
1j
kjk
s
)s|P(v log  )P(s logargmaxs*
k
)s,count(v
)s|P(v
t
kt
kj
ïƒ¥
)count(word
)count(s
)P(s kk ï€½
89
Example
ï® Training corpus (context window = Â±3 words):
â€¦Today the World Bank/BANK1 and partners are calling for greater reliefâ€¦
â€¦Welcome to the Bank/BANK1 of America the nation's leading financial institutionâ€¦
â€¦Welcome to America's Job Bank/BANK1 Visit our site andâ€¦
â€¦Web site of the European Central Bank/BANK1 located in Frankfurtâ€¦
â€¦The Asian Development Bank/BANK1 ADB a multilateral development financeâ€¦
â€¦lounging against verdant banks/BANK2 carving out the...
â€¦for swimming, had warned her off the banks/BANK2 of the Potomac. Nobody...
ï® Training:
ï± P(the|BANK1) = 5/30 P(the|BANK2) = 3/12
ï± P(world|BANK1) = 1/30 P(world|BANK2) = 0/12
ï± P(and|BANK1) = 1/30 P(and|BANK2) = 0/12
ï± â€¦ â€¦
ï± P(off|BANK1) = 0/30 P(off|BANK2) = 1/12
ï± P(Potomac|BANK1) = 0/30 P(Potomac|BANK2) = 1/12
ï± P(BANK1) = 5/7 P(BANK2) = 2/7
ï® Disambiguation: â€œI lost my left shoe on the banks of the river Nile.â€
ï± Score(BANK1)=log(5/7) + log(P(shoe|BANK1))+log(P(on|BANK1))+log(P(the|BANK1)) â€¦
ï± Score(BANK2)=log(2/7) + log(P(shoe|BANK2)+log(P(on|BANK2))+log(P(the|BANK2)) â€¦
BANK1 BANK2
90
Example (with add 0.5 smoothing)
ï® Assume V = 50
ï± P(the|BANK1) = (5+.5) / (30+.5V) P(the|BANK2) = (3+.5) / (12 + .5V)
ï± P(world|BANK1) = (1+.5) / 55  P(world|BANK2) = (0+.5) / 37
ï± P(and|BANK1) = (1+.5) / 55  P(and|BANK2) = (0+.5) / 37
ï± â€¦
ï± P(off|BANK1) = (0+.5) / 55 P(off|BANK2) = (1+.5) / 37
ï± P(Potomac|BANK1) = (0+.5) / 55 P(Potomac|BANK2) = (1+.5) / 37
ï± Score(BANK2)=log(2/7) + log(P(shoe|BANK2))+log(P(on|BANK2))+log(P(the|BANK2)) â€¦
â†’ Worksheet #10 (â€œWord Sense Disambiguationâ€)
91
Stages of NL Understanding
92
Compositional Semantics
ï® The cat eats the mouse = The mouse is eaten by the cat.
ï® Goal:
ï± map an expression into a knowledge representation
ï® a representation of context-independent, literal meaning
ï® e.g. first-order predicate logic, conceptual graph, embedding...
ï± to assign semantic roles (different from grammatical roles):
ï® Semantic roles: Agent, Patient, Instrument, Time,  Location, â€¦
ï® Grammatical roles:  subject, direct object, ...
ï® E.g.
ï± The child hid the candy under the bed.
Hide (agent=child, patient=candy,
location=under_the_bed, time=past)
93
Some Difficulties
ï® Syntax is not enough
ï± I ate spaghetti with a fork.  <instrument>
ï± I ate spaghetti with my sister.     <accompanying person>
ï± I ate spaghetti with meat balls.  <attribute of food>
ï± I ate spaghetti with lots of appetite. <manner>
ï± Gun = instrument that can kill
ï± Metal gunâ€¦ a gun made out of metal
ï± Water gunâ€¦ a gun made out of water?
ï± Fake gunâ€¦ it is a gun anyways?  Can it kill?
ï± General Kaneâ€¦ person     but  General Motors â€¦ corporation
ï® Parallel problems to syntactic ambiguity
ï± Happy [cats and dogs] live on the farm
ï± [Happy cats] and dogs live on the farm
ï® Quantifier Scoping
ï± Every man loves a woman.
94
95
Discourse Analysis
ï® In logics:
ï® Not in NL:
ï± John visited Paris.  He bought Mary some expensive
perfume.  Then he flew home.  He went to Walmart.  He
bought some underwear.
ï± John visited Paris. Then he flew home. He went to
Walmart. He bought Mary some expensive perfume. He
ï® Humans infer relations between sentences that may not
be explicitly stated in order to make a text coherent.
ï± (?) I am going to Concordia.  I need butter.
96
Examples of Discourse Relations
CONDITION  If it rains, I will go out.
SEQUENCE Do this, then do that.
CONTRAST This is good, but this is better.
CAUSE Because I was sick, I could not do my assignment.
RESULT  Click on the button, the red light will blink.
PURPOSE To use the computer, get an access code.
ELABORATION The solution was developed by Alan Turing.
Turing was a great mathematician living in
Great Britain. He was an atheist as well as gay.
Another Classification Problem,  again!
ï®  Discourse tagging can be viewed as typical classification problem
ï® use machine learning techniques (ex. NaÃ¯ve Bayes classifier, decision
tree) to train a system
ï® that learns a classifier to assign to unseen sentences one of a fixed
number of discourse relations (categories)
ï®  Input:
ï® Sentence  Ex. If it rains, I will go out.
ï® Features?
ï‚§ Connectives such as â€œifâ€, â€œhoweverâ€, â€œin conclusionâ€
ï‚§ Tense of verb (future, past)
ï‚§ â€¦
ï® Most likely relation in the sentence (none, condition, contrast,
purpose, â€¦)
98
99
Pragmatics
ï® goes beyond the literal meaning of a sentence
ï® tries to explain what the speaker is really expressing
ï® understanding how people use language socially
ï± E.g.: figures of speech, â€¦
ï± E.g.: Could you spare some change?
100
101
Using World Knowledge
ï® Using our general knowledge of the world to interpret
a sentence/discourse
The trophy would not fit in the brown suitcase because ...
... it was too big.
... it was too small.
The professor sent the student to see the principal becauseâ€¦
â€¦he wanted to see him.
â€¦he was throwing paper balls in class.
â€¦he could not take it anymore.
ï± Ex: Silence of the lambsâ€¦
Current Research area: see Winograd Schema Challenge
https://www.youtube.com/watch?v=sbJ89LFheTs
https://en.wikipedia.org/wiki/Winograd_Schema_Challenge
102
Summary of NLU
World Knowledge
104
to see in a few classes
Slide 1
Slide 2
Slide 3
Slide 4
Slide 5
Slide 6
Slide 7
Slide 8
Slide 9
Slide 10
Slide 11
Slide 12
Slide 13
Slide 14
Slide 15
Slide 16
Slide 17
Slide 18
Slide 19
Slide 20
Slide 21
Slide 22
Slide 23
Slide 24
Slide 25
Slide 26
Slide 27
Slide 28
Slide 29
Slide 30
Slide 31
Slide 32
Slide 33
Slide 34
Slide 35
Slide 36
Slide 37
Slide 38
Slide 39
Slide 40
Slide 42
Slide 43
Slide 44
Slide 45
Slide 46
Slide 47
Slide 48
Slide 49
Slide 50
Slide 51
Slide 52
Slide 53
Slide 54
Slide 55
Slide 56
Slide 57
Slide 58
Slide 59
Slide 60
Slide 61
Slide 62
Slide 63
Slide 64
Slide 65
Slide 66
Slide 67
Slide 68
Slide 70
Slide 71
Slide 72
Slide 73
Slide 74
Slide 75
Slide 76
Slide 77
Slide 78
Slide 79
Slide 80
Slide 81
Slide 82
Slide 83
Slide 84
Slide 85
Slide 86
Slide 87
Slide 88
Slide 89
Slide 90
Slide 91
Slide 92
Slide 93
Slide 94
Slide 95
Slide 96
Slide 97
Slide 98
Slide 99
Slide 100
Slide 101
Slide 102
Slide 104
