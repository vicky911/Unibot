COMP 6721: AI
Artificial Intelligence:
Deep Learning for NLP
many slides from:  Y. Bengio, A. Ng and Y. LeCun
1
Today
1. Introduction
2. Word Embeddings
3. Deep Learning for NLP
2
History of AI
3
Deep Learning in the News (2013)
Slide from Yoshua Bengio, 2015 4
Deep Learning in the News (2012-2014)
Slide from Yoshua Bengio, 2015 5
Major Breakthroughs
Image Captioning (deep vision + deep NLP)
Question Answering
 Speech Recognition & Machine Translation (2010+)
 Image Recognition & Computer Vision (2012+)
 Natural Language Processing (2014+)
6
A
B
Image Captioning: Better than humans?
7
Many Types of Neural Networks
https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b4638
Autoencoder
 The network is trained
to output the input
 i.e. learn the identity
function.
 Trivial… unless, we
impose constraints:
 Nb of units in layer 2 < nb
of input units (learn
compressed
representation)
OR
 Constrain layer 2 to be
sparse (i.e. many
connections are
“disabled”)
x4
x5
x6
+1
layer 1
(input)
layer 2
x1
x2
x3
layer 3
(output)
a1
a2
a3
9
input
layer
Feedforward input
Backprop error
output
10
11
New (compressed) representation of
the input to be fed to the next layer
i.e. the encoded x
input layer
12
1. Motivation
13
Deep Learning for NLP
Deep learning models for  NLP use:
 Vector representation of words
 i.e., word embeddings
 Neural network structures
 Recurrent Neural Networks (RNNs)
 Convolutional Networks (CNNs)
 Recursive Neural Networks
 …
14
Word Embeddings
 To do NLP with neural networks, words need to be represented
as vectors
 Traditional approach: “one hot vector”
 Binary vector
 Length = | vocab |
 1 in the position of the word id, the rest are 0
 [0, 0, 0, 1, 0, 0, 0, . . .]
 However, this does not represent word meaning ;-(
http://ahogrammer.com/2017/03/22/why-is-word-embeddings-important-for-natural-language-processing/
 Similar words such as python and ruby
should have similar vector representations
 However, similarity/distance between all
“one hot vectors” is the same
15
 We would like:
 cat/kitty/dog ... to have similar representations
 cat/orange/train/python… to have dissimilar representations
 Word embeddings:
 aka. word representations
 Represent each word by a vector of fixed dimensions
(eg, n= 50 to 300)
 Like a point in n-dimensional space
16
Word2vec
 Popular embedding method
 Very fast to train
 Code available on the web
 https://code.google.com/archive/p/word2vec
 Idea:
 predict rather than count
 use unsupervised texts from the Web
17
https://code.google.com/archive/p/word2vec/
 Instead of counting how often each word w occurs near
"apricot"
 Train a classifier on a binary prediction task:
 Is w likely to show up near “cat"?
 In the end, we do not actually care about this task
 But we will take the learned weights as the word embeddings
 Use as training set readily available texts, so no need for
hand-labeled supervision !
18
Word2vec Models
 Basic Idea:
1. Similar words should have similar contexts (surrounding words)
2. So we can use the contexts to guess the word (or vice-versa).
 The cat/kitty/dog hunts for mice.
 The brown furry cat/kitty/dog  is eating.
 John’s cat/kitty purrs.
3. Train an ANN to guess a word given its context (or vice-versa)
“A word is known by the
company it keeps” – J. R. Firth
19
Word2Vec models
Word2Vec has 2 models:
1. CBOW:  given
context words, guess
the word
2. Skip-gram: given a
word, guess one of
its surrounding word
http://rohanvarma.me/Word2Vec/
20
Word2Vec: CBOW Model
Uses a shallow neural network
with only 3 layers:
1. One input layer
2. One hidden layer and
3. One output layer.
goal: predict the probability of
a target word (cat) given a
context (brown furry chases
the).
The brown furry cat chases the mouse
on
e-
ho
t
ve
ct
or
fo
r
“b
ro
w
n”
“f
ur
ry
”
“t
he
… P
ba
bi
lit
y
of
d
“c
at
http://www.claudiobellei.com/2018/01/06/backprop-word2vec/ 21
Word2Vec – Creating the Data Set
The brown furry cat chases the mouse inside the house.
…
...
Instance Context
word -2
Context
word -1
word +1
word +2
To
Predict
1 the brown cat chases furry
2 brown furry chases the cat
3 furry cat the mouse chases
4 cat chases mouse inside the
5 chases the inside the mouse
6 the mouse the house inside
Assume context words are
those in +/- 2 word window
22
Word2Vec – Input to the Network
n
ha
se
s”
Pr
ob
ab
ili
ty
o
f
V = size of vocabulary
N = size of the embedding that we want
(i.e. number of neurons in the hidden layer)
C = size of context (2 words before + 2 after)
Inst
ance
Input word
1 Context -2 the 0 1 0 0 0 0 0 0 0 …
1 Context -1 brown 0 0 0 0 0 1 0 0 0 …
1 Context +1 cat 1 0 0 0 0 0 0 0 0 …
1 Context +2 chases 0 0 0 1 0 0 0 0 0
V
2 Context -2 brown 0 0 0 0 0 1 0 0 0 …
2 Context -1 furry 0 0 0 0 1 0 0 0 0 …
2 Context +1 chases 0 0 0 0 0 0 0 0 1 …
Context +2 the 0 1 0 0 0 0 0 0 0 ……
23
Word2Vec – Weights W
“
ca
t”
 Weight Matrix W between input & hidden
 W is a VxN matrix…
 Initially random but modified via backprop
N (nb of nodes in hidden layer, i.e. size of the embedding)
24
1 2 3 4 5 ...
6 7 8 9 10 ...
11 12 13 14 15 ...
16 17 18 19 20 ...
21 22 23 24 25 ...
26 27 28 29 30 ...
31 32 33 34 35 ...
36 37 38 39 40 ...
41 42 43 44 45 ...
46 47 48 49 50 ...
... ... ... ... ... ...
Word2Vec – Feedforward
1. Calculate the output of each of the N hidden nodes
for each context word
Note: there is no activation function.
Output of hi is just the dot product
2. Then take the average
=.
N
C
25
instance 1 Context -2 the 0 1 0 0 0 0 0 0 0 ...
instance 1 Context -1 brown 0 0 0 0 0 1 0 0 0 ...
instance 1 Context +1 cat 0 0 0 0 0 0 0 0 1 ...
instance 1 Context +2 chases 0 0 0 0 1 0 0 0 0 ...
h1 h2 h3 h4 h5 ...
average of dot products
h1 h2 h3 h4 h5
24 25 26 27 28 …
Word2Vec – Weights W’
Matrix W’
.
N (size of embeddings)
 Weight Matrix W’ between hidden & output layer
 W’ is a NxV matrix…
 Feed forward average of hidden neurons and do
dot product with matrix W’
=
26
0.70 0.13 0.14 0.60 0.16 0.17 0.60 0.19 0.20 0.21 …
0.12 0.50 0.14 0.15 0.50 0.30 0.18 0.20 0.20 0.21 …
0.17 0.40 0.60 0.25 0.26 0.50 0.35 0.37 0.30 0.55 …
0.32 0.33 0.34 0.38 0.36 0.37 0.25 0.55 0.56 0.41 …
0.42 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.50 0.30 …
… … … … … … … … … … …
y1 y2 y3 y4 y5 y6 y7 y8 y9 y10
output layer 43.7550 46.0750 43.1300 46.5950 44.8300 46.8250 47.2600 46.8500 45.8400 43.2200 …
Turn dot product into probabilities
// use softmax function
// to turn the vector above into
// a vector of probabilities
// (that nicely add up to 1)
27
output layer 43.7550 46.0750 43.1300 46.5950 44.8300 46.8250 47.2600 46.8500 45.8400 43.2200 … sum
e^y_i 1.01E+19 1.02E+20 5.38E+18 1.72E+20 2.95E+19 2.17E+20 3.35E+20 2.22E+20 8.09E+19 5.89E+18 … 1.180E+21
softmax(y_i) 0.0085 0.0868 0.0046 0.1459 0.0250 0.1837 0.2837 0.1883 0.0686 0.0050 … 1.0000
Compute Error of output layer
-
To Predict
remember the training set:
t1 t2 t3 t4 t5 t6 t7 t8 t9 t10
// target = 1 hot
representation
of the target
(furry) in the
training set
28
O-T 0.0085 0.0868 0.0046 0.1459 0.0250 0.1837 -0.7163 0.1883 0.0686 0.0050 …
O= output 0.0085 0.0868 0.0046 0.1459 0.0250 0.1837 0.2837 0.1883 0.0686 0.0050 …
T= target 0 0 0 0 0 0 1 0 0 0 …
Backpropagate errors to adjust W and W’
 Adjust W’ and W using backpropagation
 <after a bit of math>, we get:
η: learning rate
C: size of context (eg 4)
29http://www.stokastik.in/understanding-word-vectors-and-word2vec/
update only the wij for the inputs where x1 =1
The weight updates are ONLY done on the "rows" of W that correspond to
the input word, not for all elements of W. Remember that the input words
are represented as 1-hot vectors, so only the weights of the word that has
a "1" are updated.  This makes intuitive sense, as we want to update the
weight only of the context word since its previous weights lead to an error.
http://www.stokastik.in/understanding-word-vectors-and-word2vec/
Word2Vec – FeedForward next data
1 Context +2 the 0 1 0 0 0 0 0 0 0 …
 Iterate feedforward/ backprop until error is minimized
 Trained on Google News dataset (about 100 billion words).
 See: https://code.google.com/archive/p/word2vec/
30
almost...
remember, we did all this to get embeddings...
I’m not leaving ‘till I get my embeddings!
31
Word2Vec- Get the embeddings
 After many iterations of feedforward, backpropagation on the entire
training set ...
 The classifier will be built!
 Then, we throw it away ! (yes, we do!)
 and only keep W’, these are your word embeddings!
Embedding for
banana
Embedding
for airplane
for zoo
32
Results
vector[queen] ~ vector[king]  - vector[man] + vector[woman]
vector[swimming] ~ vector[swam]  - vector[walked] + vector[walking]
vector[Rome] ~ vector[Madrid] – vector[Spain] + vector[Italy]
33
34
35
Multilingual Word Embeddings
Used in Machine Translation
Words in different
languages but with
similar meanings
(i.e. translations)
are represented by
similar vectors
36
Word History through Embeddings
Train embeddings on old books to study
changes in word meaning
~30 million books, 1850-1990, Google Books data
Project 300 dimensions down into 2
37
38
39
Stages of NLU
source: Luger (2005)
40
Recurrent Neural Networks
 To model sequences of decisions, such as machine
translation, language modelling, ….
 e.g., A word at position n can influence a word/decision at
position n+t
 decision/output from the past can influence current
decision/output
 Networks with loops in them, allowing information to persist.
http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/
41
Cool Applications
 RNNs + word embeddings
 Google Translate
 As of July 2017, uses an RNN + word embeddings (called Neural
Machine Translation (NMT) )
 Input sequence: words in a source sentence
 Output sequence: words in the target language
 Dialogue Systems
 CNNs + RNNs + word embeddings
 Image Captioning
 Video to Text Descriptions
 Visual Question Answering
 …
42
https://translate.google.ca/
https://visualdialog.org/
http://deeplearning.cs.toronto.edu/i2t
https://vsubhashini.github.io/s2vt.html
Demo Website
Input: an image + a natural language question
Output: natural language answer
Visual Question Answering
Picture from (Antol et al., 2015) 43
http://visualqa.csail.mit.edu/
 The output is conditioned on both image and textual inputs.
 A CNN is used to encode the image.
 A RNN is used to encode the sentence.
Visual Question Answering
3844
Machine Translation
45
Conversational Agents
http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/ 46
47
